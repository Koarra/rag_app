# KMPI Monitoring System - Team Presentation

## Introduction to KMPIs

We have implemented a Key Model Performance Indicator system to continuously monitor the quality and performance of our document analysis model. KMPIs are formal metrics that we must track and report on regularly to ensure our model maintains acceptable standards. We have five KMPIs in total, though only four are automated.

## The KMPIs We're Monitoring - Detailed Metrics

**KMPI #1 and KMPI #2** focus on extraction quality and are checked quarterly. Let me explain these in detail because they're the core of our quality monitoring.

**Entity Recall (KMPI #1)** measures how well our model detects and extracts entities from documents compared to a known reference set. Our existing compare_outputs.py script calculates this using intersection over union (also known as the Jaccard similarity coefficient or IoU). For each test article, we have reference outputs that represent the "ground truth" - the entities that should be detected. When we run our model, we normalize both the reference entities and the detected entities by converting them to lowercase and combining the entity name with its type (for example, "john smith|person" or "deutsche bank|organization"). The calculation is straightforward: we count how many entities appear in both the reference and our output (the intersection), then divide by the total number of unique entities across both sets (the union). This gives us a score between 0 and 1, where 1 means perfect recall and 0 means we detected nothing correctly. We need to maintain at least 85% similarity, meaning our model must consistently identify at least 85% of the expected entities. This KMPI is evaluated by looking back at the last three monthly test executions every quarter. If any single month falls below the 85% threshold, the entire KMPI is reported as FAILED and triggers a mandatory root-cause analysis.

The entity matching is strict. We don't just match on names - we also match on entity types. So if the reference says "John Smith" is a PERSON but our model detects "John Smith" as an ORGANIZATION, that counts as a miss. This ensures we're not just finding text, but correctly classifying what we find. The intersection-over-union metric accounts for both false negatives (entities we missed) and false positives (entities we incorrectly added) because both decrease the similarity score. If we have 10 reference entities and detect 8 correctly but also add 2 wrong ones, we have 8 in the intersection and 12 in the union, giving us 67% similarity.

**Crime Recall (KMPI #2)** measures how accurately the model flags entities with specific crimes once an entity has been correctly identified. This is a secondary metric that only applies to entities that were successfully matched between our output and the reference. For each matched entity, we compare the set of crimes flagged by our model against the reference set of crimes. Again, our existing code uses intersection over union: we count crimes that appear in both sets, divide by total unique crimes across both sets. We then average this score across all matched entities to get an overall crime recall percentage. The 85% threshold means that on average, for the entities we correctly identify, we must also correctly classify at least 85% of their associated crimes. This is crucial because getting the entity right but missing the crimes defeats the purpose of the analysis. Like KMPI #1, this is evaluated quarterly over three months, and a single month below threshold causes failure.

The crime matching is also exact. If the reference says an entity is flagged for "money laundering" and "fraud," but we only detect "money laundering," we have 1 crime in the intersection and 2 in the union, giving us 50% for that entity. If we detect "money laundering," "fraud," and incorrectly add "tax evasion," we have 2 in the intersection but 3 in the union, giving us 67%. This penalizes both under-detection and over-detection of crimes.

**KMPI #3** is the User SME feedback mechanism, measured twice per year. This requires a Subject Matter Expert to manually review a sample of at least ten cases and determine if they were processed as expected. We need 80% or more of cases to be deemed acceptable for this KMPI to pass. This one is not automated and requires manual coordination with the SME team. It serves as a human validation layer to catch issues that automated metrics might miss, such as context understanding or practical usability problems.

**KMPI #4** tracks article processing productivity and is evaluated twice per year. We're monitoring whether the model helps people process more articles without overwhelming them. Our baseline is 200 articles per person per month, which represents the current manual processing capacity. The model is expected to increase productivity, but if any month shows more than a 100% increase (meaning over 400 articles per person per month), the KMPI fails and triggers an IRR re-assessment. This ensures we catch early warning signs if the workload becomes unsustainable. The metric is straightforward: we count total articles processed in production during a month and divide by the number of active users. This gives us an average that we track over time. We monitor six consecutive months, and if any single month exceeds the 400-article limit, we need to investigate whether people are being overwhelmed or if there are quality shortcuts being taken.

**KMPI #5** is similar to KMPI #1 but uses a slightly lower threshold of 83% for entity recall. It uses the exact same calculation methodology as KMPI #1 - intersection over union on normalized entity keys. The lower threshold might seem redundant, but it provides an early warning system. We might pass the 83% threshold while still failing the 85% threshold, which gives us nuanced insight into performance degradation. It's also evaluated quarterly using the same three-month lookback period. Like the others, if any single month falls below 83%, the KMPI fails.

## Technical Implementation Details

Our testing infrastructure runs against a set of five benchmark articles that represent typical real-world documents. For each article, we have stored reference outputs in JSON format that contain the expected entities and their associated crimes. The reference outputs were created by expert review and represent the gold standard for what the model should detect. When we run monthly tests, we process these same five articles through our full analysis pipeline, which includes the model that performs entity extraction and crime flagging. We then use our existing compare_outputs.py module to calculate the similarity metrics.

The comparison logic normalizes entities by creating a key that combines the entity name (lowercased and stripped of whitespace) with the entity type. This means "Deutsche Bank" as an ORGANIZATION creates the key "deutsche bank|organization". We do this for both reference and current outputs, creating two sets of keys. The entity similarity calculation takes the intersection of these sets (entities found in both), divides by the union (all unique entities across both sets), giving us a percentage. For the articles we test, we typically have between 10 and 30 entities per article, so our similarity calculations are based on meaningful sample sizes.

For crime recall, we only look at entities that appear in both the reference and current output (the matched set). For each matched entity, we extract the list of crimes from both sources. These are already normalized strings like "money laundering" or "fraud". We calculate intersection over union for each entity's crime set, then average across all matched entities. This means if we match 15 entities and each has a crime similarity of 90%, 85%, 80%, etc., we average those 15 scores to get the overall crime recall.

The reason we separate entity recall from crime recall is because they represent different failure modes. Low entity recall means the model is missing entities entirely or hallucinating entities that don't exist. Low crime recall means the model finds the right entities but misclassifies their associated crimes. Both are critical failures, but they require different debugging approaches.

## Our Solution: Automated Monitoring with Cron

To implement this monitoring system, we've built an automated solution using cron jobs that run on a scheduled basis without manual intervention. The system consists of three main components that work together to collect data, evaluate thresholds, and generate reports.

The first component runs monthly on the first day of each month at 2 AM. This is our monthly test script, which executes the existing performance tests against our benchmark articles. It calls our run_test.py script which performs the full test suite: processing each article, generating outputs, comparing against references using compare_outputs.py, and calculating both entity and crime similarity scores. The results include not just the aggregate percentages but also detailed breakdowns of which entities were missed, which were added incorrectly, and which crimes were misclassified. All of this gets saved to a JSON file named with the year and month, like monthly_202501.json. These monthly results become the historical data that our quarterly reports will analyze. This script leverages all the existing test infrastructure you've already built, so we're not reinventing the wheel - we're just wrapping it in a scheduled execution layer.

The second component runs quarterly on the first day of January, April, July, and October at 3 AM. This quarterly report script reads the last three monthly test results and evaluates them against our KMPI thresholds. It loads the three most recent monthly JSON files, extracts the entity and crime similarity scores from each, and checks them against three thresholds: entity at 85%, crime at 85%, and entity at 83%. The logic is strict and follows the KMPI requirements exactly. For each KMPI, it iterates through the three months and identifies any that fell below the threshold. If all three months pass all thresholds, the KMPIs are reported as PASSED. If any single month fails any threshold, that KMPI is reported as FAILED. The script then saves a comprehensive report showing which KMPIs passed or failed, includes the monthly values for transparency, and lists specifically which months failed and by how much. This report becomes the official quarterly KMPI record.

The third component runs twice per year on January 1st and July 1st at 4 AM. This is our productivity monitoring script for KMPI #4. It reads production metrics from a JSON file that tracks how many articles each person processes per month. The file contains an array of monthly records with the month identifier and the average articles per person for that month. The script loads this file, takes the last six entries, and checks whether any month exceeded our 400 articles per person limit. If all six months stayed within the limit, the KMPI passes. If any single month exceeded the limit, the KMPI fails and the report indicates that an IRR re-assessment is required. The report includes the full six-month history so we can see trends over time.

## What the Team Needs to Do

There's one critical requirement for this system to work properly: we must maintain production metrics. Every month, someone needs to update the file located at production_metrics/productivity.json with the average number of articles processed per person. This requires tracking the total articles processed in production and dividing by the number of active users. The calculation is straightforward: if we processed 5,250 articles in January and had 25 active users, the average is 210 articles per person. This needs to be appended to the JSON array with the month identifier. Without this data, the bi-annual productivity report cannot run and KMPI #4 cannot be evaluated. We should establish a process where this is done as part of our monthly operational review.

The installation is simple. We copy four Python scripts into our test_performance directory: config.py (updated with KMPI thresholds), monthly_test.py, quarterly_report.py, and biannual_productivity.py. Then we add three cron job entries to the system crontab. A setup script is provided that shows exactly what commands to add. Once installed, the system runs automatically and requires no intervention unless a KMPI fails. The scripts are written in straightforward Python with minimal dependencies, using only the standard library plus what's already in our project.

## Monitoring and Response

All execution logs are written to dedicated log files so we can monitor the health of the system. The cron jobs redirect both stdout and stderr to log files: monthly.log, quarterly.log, and productivity.log. These logs show the execution timestamps, the calculated metrics, pass/fail status, and any errors encountered. When a KMPI fails, the script exits with an error code (1 instead of 0) and logs detailed information about what went wrong. For quarterly KMPIs, failure means we need to conduct a root-cause analysis to understand why the model performance degraded. This might involve reviewing the specific articles that caused issues, examining recent model changes, checking data quality, or investigating infrastructure problems. For the productivity KMPI, failure means we need to conduct an IRR re-assessment to evaluate whether the increased workload is acceptable or if we need to adjust staffing, training, or the workflow itself.

The JSON reports saved to the kmpi_reports directory provide a permanent audit trail. Each report includes the evaluation date, the months that were analyzed, the calculated metrics, pass/fail status, and details of any failures. This gives us full traceability and allows us to generate historical trend reports or demonstrate compliance during audits.

## Handling Potential Challenges

If someone asks "what if we're close to the threshold, like 84.9%," the answer is clear: we round appropriately, but the threshold is firm. The KMPI definitions don't have gray areas. If we're consistently hitting 84-85%, that's actually a warning sign that we need to investigate before we start failing regularly.

If asked "why do we fail the entire quarter if just one month is bad," the answer comes from the KMPI requirements themselves. The requirement explicitly states "if the value is calculated to be under the threshold even once during the quarter, the result of the KMPI shall be reported as FAILED." This is by design - it prevents us from averaging away problems and ensures we respond quickly to quality degradation.

If someone questions whether five test articles is enough, we can explain that these are carefully selected benchmark articles that represent diverse scenarios. The same five articles are used consistently, which means we're measuring stability and consistency over time. If we detect issues on these five, it's a strong signal that production is affected too. We can always expand the benchmark set if needed.

If there are concerns about the intersection-over-union metric being too strict, we can explain that it's actually a balanced metric. It penalizes both false positives and false negatives equally, which is exactly what we want. Alternative metrics like precision or recall alone would only measure one side of the problem. Intersection over union gives us a single number that captures overall quality, treating missed entities and extra entities with equal weight.

The key principle behind all of this is strict adherence to the KMPI definitions: one bad month means the entire KMPI fails, period. There are no exceptions or averaging. This ensures we catch problems early and take corrective action before they become systemic issues. The system is designed to be conservative and favor early detection over convenience.
